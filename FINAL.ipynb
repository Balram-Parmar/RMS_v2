{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc60c9ab",
   "metadata": {},
   "source": [
    "# Class For DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73c2b7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full pipeline...\n",
      "\n",
      "Data loaded successfully: 68520 rows, 13 columns\n",
      "Data preprocessed: Train size = 54816, Test size = 13704\n",
      "Model trained successfully!\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "==================================================\n",
      "Accuracy:  0.7183\n",
      "Precision: 0.7245\n",
      "Recall:    0.6942\n",
      "F1-Score:  0.7090\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5141 1788]\n",
      " [2072 4703]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Disease       0.71      0.74      0.73      6929\n",
      "     Disease       0.72      0.69      0.71      6775\n",
      "\n",
      "    accuracy                           0.72     13704\n",
      "   macro avg       0.72      0.72      0.72     13704\n",
      "weighted avg       0.72      0.72      0.72     13704\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model trained successfully!\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "==================================================\n",
      "Accuracy:  0.7183\n",
      "Precision: 0.7245\n",
      "Recall:    0.6942\n",
      "F1-Score:  0.7090\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5141 1788]\n",
      " [2072 4703]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Disease       0.71      0.74      0.73      6929\n",
      "     Disease       0.72      0.69      0.71      6775\n",
      "\n",
      "    accuracy                           0.72     13704\n",
      "   macro avg       0.72      0.72      0.72     13704\n",
      "weighted avg       0.72      0.72      0.72     13704\n",
      "\n",
      "==================================================\n",
      "\n",
      "Predictions exported to: cardio_train_pred.csv\n",
      "Pipeline completed successfully!\n",
      "Predictions exported to: cardio_train_pred.csv\n",
      "Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CardioDecisionTreeClassifier:\n",
    "    \"\"\"\n",
    "    A reusable Decision Tree classifier for cardiovascular disease prediction.\n",
    "    \n",
    "    Features:\n",
    "    - Train/test split with customizable ratio\n",
    "    - Optional feature scaling\n",
    "    - Comprehensive performance metrics\n",
    "    - Export predictions to CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_size=0.2, random_state=42, max_depth=None,\n",
    "                 min_samples_split=2, min_samples_leaf=1, criterion='gini',\n",
    "                 class_weight='balanced', scale_features=True,\n",
    "                 target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float, default=0.2\n",
    "            Proportion of dataset to include in test split\n",
    "        random_state : int, default=42\n",
    "            Random state for reproducibility\n",
    "        max_depth : int, default=None\n",
    "            Maximum depth of the tree\n",
    "        min_samples_split : int, default=2\n",
    "            Minimum samples required to split a node\n",
    "        min_samples_leaf : int, default=1\n",
    "            Minimum samples required at a leaf node\n",
    "        criterion : str, default='gini'\n",
    "            The function to measure the quality of a split\n",
    "        class_weight : str or dict, default='balanced'\n",
    "            Weights associated with classes\n",
    "        scale_features : bool, default=True\n",
    "            Whether to scale features\n",
    "        target_col : str, default='cardio'\n",
    "            Name of the target variable column\n",
    "        drop_cols : list, default=None\n",
    "            List of columns to drop (e.g., ID columns)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.criterion = criterion\n",
    "        self.class_weight = class_weight\n",
    "        self.scale_features = scale_features\n",
    "        self.target_col = target_col\n",
    "        self.drop_cols = drop_cols if drop_cols else []\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.data = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.feature_names = None\n",
    "        self.metrics = {}\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load data from CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Loaded dataframe\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        print(f\"Data loaded successfully: {self.data.shape[0]} rows, {self.data.shape[1]} columns\")\n",
    "        return self.data\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess the data for training.\n",
    "        Uses `self.target_col` and `self.drop_cols` from initialization.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        \n",
    "        df = self.data.copy()\n",
    "        if self.drop_cols:\n",
    "            df = df.drop(columns=self.drop_cols, errors='ignore')\n",
    "        \n",
    "        X = df.drop(columns=[self.target_col])\n",
    "        y = df[self.target_col]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        if self.scale_features:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X_train = pd.DataFrame(\n",
    "                self.scaler.fit_transform(self.X_train),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_train.index\n",
    "            )\n",
    "            self.X_test = pd.DataFrame(\n",
    "                self.scaler.transform(self.X_test),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_test.index\n",
    "            )\n",
    "        \n",
    "        print(f\"Data preprocessed: Train size = {len(self.X_train)}, Test size = {len(self.X_test)}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the Decision Tree classifier.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not preprocessed. Call preprocess_data() first.\")\n",
    "        \n",
    "        self.model = DecisionTreeClassifier(\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            criterion=self.criterion,\n",
    "            class_weight=self.class_weight,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        print(\"Model trained successfully!\")\n",
    "    \n",
    "    def predict(self, X=None):\n",
    "        \"\"\"\n",
    "        Make predictions on data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame, default=None\n",
    "            Data to predict. If None, uses test set.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        if X is None:\n",
    "            X_to_predict = self.X_test\n",
    "        else:\n",
    "            X_to_predict = X\n",
    "\n",
    "        if X is not None and self.scale_features and self.scaler is not None:\n",
    "            X_to_predict = pd.DataFrame(\n",
    "                self.scaler.transform(X_to_predict),\n",
    "                columns=X_to_predict.columns,\n",
    "                index=X_to_predict.index\n",
    "            )\n",
    "        \n",
    "        return self.model.predict(X_to_predict)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test set.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing all performance metrics\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        y_pred = self.predict()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "            'precision': precision_score(self.y_test, y_pred, average='binary'),\n",
    "            'recall': recall_score(self.y_test, y_pred, average='binary'),\n",
    "            'f1_score': f1_score(self.y_test, y_pred, average='binary'),\n",
    "            'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL PERFORMANCE METRICS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Accuracy:  {self.metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {self.metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {self.metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {self.metrics['f1_score']:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(self.metrics['confusion_matrix'])\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred, \n",
    "                                      target_names=['No Disease', 'Disease']))\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def export_predictions(self, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Export predictions for the *entire* loaded dataset to a new CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_filepath : str\n",
    "            Path for the output CSV file\n",
    "        include_all_data : bool, default=True\n",
    "            If True, includes all original columns. If False, only includes\n",
    "            the target and predicted columns.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "            \n",
    "        \n",
    "        # Prepare the full dataset for prediction\n",
    "        X_full = self.data.copy()\n",
    "        \n",
    "        if self.drop_cols:\n",
    "            X_full = X_full.drop(columns=self.drop_cols, errors='ignore')\n",
    "            \n",
    "        X_full = X_full.drop(columns=[self.target_col], errors='ignore')\n",
    "        \n",
    "        # Ensure columns match training data\n",
    "        X_full = X_full[self.feature_names]\n",
    "        \n",
    "        if self.scale_features and self.scaler is not None:\n",
    "            X_full_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X_full),\n",
    "                columns=self.feature_names,\n",
    "                index=X_full.index\n",
    "            )\n",
    "            predictions = self.model.predict(X_full_scaled)\n",
    "        else:\n",
    "            predictions = self.model.predict(X_full)\n",
    "        \n",
    "        if include_all_data:\n",
    "            output_df = self.data.copy()\n",
    "        else:\n",
    "            output_df = pd.DataFrame()\n",
    "            output_df[self.target_col] = self.data[self.target_col]\n",
    "        \n",
    "        output_df['predicted'] = predictions\n",
    "        \n",
    "        output_df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Predictions exported to: {output_filepath}\")\n",
    "        \n",
    "        return output_df\n",
    "    \n",
    "    def run_full_pipeline(self, input_filepath, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline: load, preprocess, train, evaluate, and export.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_filepath : str\n",
    "            Path to input CSV file\n",
    "        output_filepath : str\n",
    "            Path for output CSV file with predictions\n",
    "        include_all_data : bool, default=True\n",
    "            Passed to export_predictions to control output file content.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Starting full pipeline...\\n\")\n",
    "        \n",
    "        self.load_data(input_filepath)\n",
    "        self.preprocess_data()\n",
    "        self.train()\n",
    "        metrics = self.evaluate()\n",
    "        self.export_predictions(output_filepath, include_all_data=include_all_data)\n",
    "        \n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure the model with all parameters during initialization\n",
    "    clf = CardioDecisionTreeClassifier(\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        max_depth=15,\n",
    "        min_samples_split=50,\n",
    "        min_samples_leaf=20,\n",
    "        criterion='gini',\n",
    "        class_weight='balanced',\n",
    "        scale_features=True,\n",
    "        target_col='cardio',  # <--- Parameter moved here\n",
    "        drop_cols=['id']      # <--- Parameter moved here\n",
    "    )\n",
    "    \n",
    "    # Run the pipeline with just the filepaths\n",
    "    matrix_decisiontree = clf.run_full_pipeline(\n",
    "        input_filepath='train.csv',\n",
    "        output_filepath='cardio_train_pred.csv'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961494d3",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "017a75b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full pipeline...\n",
      "\n",
      "Data loaded successfully: 68520 rows, 13 columns\n",
      "Data preprocessed: Train size = 54816, Test size = 13704\n",
      "Model trained successfully!\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "==================================================\n",
      "Accuracy:  0.7274\n",
      "Precision: 0.7504\n",
      "Recall:    0.6722\n",
      "F1-Score:  0.7091\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5414 1515]\n",
      " [2221 4554]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Disease       0.71      0.78      0.74      6929\n",
      "     Disease       0.75      0.67      0.71      6775\n",
      "\n",
      "    accuracy                           0.73     13704\n",
      "   macro avg       0.73      0.73      0.73     13704\n",
      "weighted avg       0.73      0.73      0.73     13704\n",
      "\n",
      "==================================================\n",
      "\n",
      "Predictions exported to: Prediction_LogisticRegression.csv\n",
      "Pipeline completed successfully!\n",
      "{'accuracy': 0.7273788674839463, 'precision': 0.750370736529906, 'recall': 0.6721771217712177, 'f1_score': 0.7091248832139521, 'confusion_matrix': array([[5414, 1515],\n",
      "       [2221, 4554]])}\n",
      "Predictions exported to: Prediction_LogisticRegression.csv\n",
      "Pipeline completed successfully!\n",
      "{'accuracy': 0.7273788674839463, 'precision': 0.750370736529906, 'recall': 0.6721771217712177, 'f1_score': 0.7091248832139521, 'confusion_matrix': array([[5414, 1515],\n",
      "       [2221, 4554]])}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CardioLogisticRegressionClassifier:\n",
    "    \"\"\"\n",
    "    A reusable Logistic Regression classifier for cardiovascular disease prediction.\n",
    "    \n",
    "    Features:\n",
    "    - Train/test split with customizable ratio\n",
    "    - Optional feature scaling\n",
    "    - Comprehensive performance metrics\n",
    "    - Export predictions to CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_size=0.2, random_state=42, C=1.0, \n",
    "                 solver='lbfgs', max_iter=1000, penalty='l2',\n",
    "                 class_weight='balanced', scale_features=True,\n",
    "                 target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float, default=0.2\n",
    "            Proportion of dataset to include in test split\n",
    "        random_state : int, default=42\n",
    "            Random state for reproducibility\n",
    "        C : float, default=1.0\n",
    "            Inverse of regularization strength; smaller values specify stronger regularization\n",
    "        solver : str, default='lbfgs'\n",
    "            Algorithm to use in the optimization problem\n",
    "        max_iter : int, default=1000\n",
    "            Maximum number of iterations taken for the solvers to converge\n",
    "        penalty : str, default='l2'\n",
    "            Specify the norm of the penalty\n",
    "        class_weight : str or dict, default='balanced'\n",
    "            Weights associated with classes\n",
    "        scale_features : bool, default=True\n",
    "            Whether to scale features\n",
    "        target_col : str, default='cardio'\n",
    "            Name of the target variable column\n",
    "        drop_cols : list, default=None\n",
    "            List of columns to drop (e.g., ID columns)\n",
    "        \"\"\"\n",
    "\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.C = C\n",
    "        self.solver = solver\n",
    "        self.max_iter = max_iter\n",
    "        self.penalty = penalty\n",
    "        self.class_weight = class_weight\n",
    "        self.scale_features = scale_features\n",
    "        self.target_col = target_col\n",
    "        self.drop_cols = drop_cols if drop_cols else []\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.data = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.feature_names = None\n",
    "        self.metrics = {}\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load data from CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Loaded dataframe\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        print(f\"Data loaded successfully: {self.data.shape[0]} rows, {self.data.shape[1]} columns\")\n",
    "        return self.data\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess the data for training.\n",
    "        Uses `self.target_col` and `self.drop_cols` from initialization.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        \n",
    "        df = self.data.copy()\n",
    "        if self.drop_cols:\n",
    "            df = df.drop(columns=self.drop_cols, errors='ignore')\n",
    "        \n",
    "        X = df.drop(columns=[self.target_col])\n",
    "        y = df[self.target_col]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        if self.scale_features:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X_train = pd.DataFrame(\n",
    "                self.scaler.fit_transform(self.X_train),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_train.index\n",
    "            )\n",
    "            self.X_test = pd.DataFrame(\n",
    "                self.scaler.transform(self.X_test),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_test.index\n",
    "            )\n",
    "        \n",
    "        print(f\"Data preprocessed: Train size = {len(self.X_train)}, Test size = {len(self.X_test)}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the Logistic Regression classifier.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not preprocessed. Call preprocess_data() first.\")\n",
    "        \n",
    "        self.model = LogisticRegression(\n",
    "            C=self.C,\n",
    "            solver=self.solver,\n",
    "            max_iter=self.max_iter,\n",
    "            penalty=self.penalty,\n",
    "            class_weight=self.class_weight,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        print(\"Model trained successfully!\")\n",
    "    \n",
    "    def predict(self, X=None):\n",
    "        \"\"\"\n",
    "        Make predictions on data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame, default=None\n",
    "            Data to predict. If None, uses test set.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        if X is None:\n",
    "            X_to_predict = self.X_test\n",
    "        else:\n",
    "            X_to_predict = X\n",
    "\n",
    "        if X is not None and self.scale_features and self.scaler is not None:\n",
    "            # When predicting on new, external data\n",
    "            X_to_predict = pd.DataFrame(\n",
    "                self.scaler.transform(X_to_predict),\n",
    "                columns=X_to_predict.columns,\n",
    "                index=X_to_predict.index\n",
    "            )\n",
    "        \n",
    "        return self.model.predict(X_to_predict)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test set.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing all performance metrics\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        y_pred = self.predict()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "            'precision': precision_score(self.y_test, y_pred, average='binary'),\n",
    "            'recall': recall_score(self.y_test, y_pred, average='binary'),\n",
    "            'f1_score': f1_score(self.y_test, y_pred, average='binary'),\n",
    "            'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL PERFORMANCE METRICS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Accuracy:  {self.metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {self.metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {self.metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {self.metrics['f1_score']:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(self.metrics['confusion_matrix'])\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred, \n",
    "                                      target_names=['No Disease', 'Disease']))\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def export_predictions(self, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Export predictions for the *entire* loaded dataset to a new CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_filepath : str\n",
    "            Path for the output CSV file\n",
    "        include_all_data : bool, default=True\n",
    "            If True, includes all original columns. If False, only includes\n",
    "            the target and predicted columns.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "            \n",
    "        \n",
    "        # Prepare the full dataset for prediction\n",
    "        X_full = self.data.copy()\n",
    "        \n",
    "        if self.drop_cols:\n",
    "            X_full = X_full.drop(columns=self.drop_cols, errors='ignore')\n",
    "            \n",
    "        X_full = X_full.drop(columns=[self.target_col], errors='ignore')\n",
    "        \n",
    "        # Ensure columns match training data\n",
    "        X_full = X_full[self.feature_names]\n",
    "        \n",
    "        if self.scale_features and self.scaler is not None:\n",
    "            X_full_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X_full),\n",
    "                columns=self.feature_names,\n",
    "                index=X_full.index\n",
    "            )\n",
    "            predictions = self.model.predict(X_full_scaled)\n",
    "        else:\n",
    "            predictions = self.model.predict(X_full)\n",
    "        \n",
    "        if include_all_data:\n",
    "            output_df = self.data.copy()\n",
    "        else:\n",
    "            output_df = pd.DataFrame()\n",
    "            output_df[self.target_col] = self.data[self.target_col]\n",
    "        \n",
    "        output_df['predicted'] = predictions\n",
    "        \n",
    "        output_df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Predictions exported to: {output_filepath}\")\n",
    "        \n",
    "        return output_df\n",
    "    \n",
    "    def run_full_pipeline(self, input_filepath, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline: load, preprocess, train, evaluate, and export.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_filepath : str\n",
    "            Path to input CSV file\n",
    "        output_filepath : str\n",
    "            Path for output CSV file with predictions\n",
    "        include_all_data : bool, default=True\n",
    "            Passed to export_predictions to control output file content.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Starting full pipeline...\\n\")\n",
    "        \n",
    "        self.load_data(input_filepath)\n",
    "        self.preprocess_data()\n",
    "        self.train()\n",
    "        metrics = self.evaluate()\n",
    "        self.export_predictions(output_filepath, include_all_data=include_all_data)\n",
    "        \n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure the model with all parameters during initialization\n",
    "    clf = CardioLogisticRegressionClassifier(\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        C=0.1,\n",
    "        solver='lbfgs',\n",
    "        max_iter=10000, # Increased max_iter for convergence\n",
    "        penalty='l2',\n",
    "        class_weight='balanced',\n",
    "        scale_features=True,\n",
    "        target_col='cardio',  # <--- Parameter moved here\n",
    "        drop_cols=['id']      # <--- Parameter moved here\n",
    "    )\n",
    "    \n",
    "    # Run the pipeline with just the filepaths\n",
    "    matrix_logisticregression = clf.run_full_pipeline(\n",
    "        input_filepath='train.csv',\n",
    "        output_filepath='Prediction_LogisticRegression.csv' # Changed output name\n",
    "    )\n",
    "    print(matrix_logisticregression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20174f7b",
   "metadata": {},
   "source": [
    "# Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45588fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full pipeline...\n",
      "\n",
      "Data loaded successfully: 68520 rows, 13 columns\n",
      "Data preprocessed: Train size = 54816, Test size = 13704\n",
      "Model trained successfully!\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "==================================================\n",
      "Accuracy:  0.7310\n",
      "Precision: 0.7448\n",
      "Recall:    0.6933\n",
      "F1-Score:  0.7181\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5320 1609]\n",
      " [2078 4697]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Disease       0.72      0.77      0.74      6929\n",
      "     Disease       0.74      0.69      0.72      6775\n",
      "\n",
      "    accuracy                           0.73     13704\n",
      "   macro avg       0.73      0.73      0.73     13704\n",
      "weighted avg       0.73      0.73      0.73     13704\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model trained successfully!\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "==================================================\n",
      "Accuracy:  0.7310\n",
      "Precision: 0.7448\n",
      "Recall:    0.6933\n",
      "F1-Score:  0.7181\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5320 1609]\n",
      " [2078 4697]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Disease       0.72      0.77      0.74      6929\n",
      "     Disease       0.74      0.69      0.72      6775\n",
      "\n",
      "    accuracy                           0.73     13704\n",
      "   macro avg       0.73      0.73      0.73     13704\n",
      "weighted avg       0.73      0.73      0.73     13704\n",
      "\n",
      "==================================================\n",
      "\n",
      "Predictions exported to: Prediction_RandomForest.csv\n",
      "Pipeline completed successfully!\n",
      "Predictions exported to: Prediction_RandomForest.csv\n",
      "Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CardioRandomForestClassifier:\n",
    "    \"\"\"\n",
    "    A reusable Random Forest classifier for cardiovascular disease prediction.\n",
    "    \n",
    "    Features:\n",
    "    - Train/test split with customizable ratio\n",
    "    - Optional feature scaling\n",
    "    - Comprehensive performance metrics\n",
    "    - Export predictions to CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_size=0.2, random_state=42, n_estimators=100, \n",
    "                 max_depth=None, min_samples_split=2, min_samples_leaf=1,\n",
    "                 max_features='sqrt', class_weight='balanced',\n",
    "                 scale_features=True, target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float, default=0.2\n",
    "            Proportion of dataset to include in test split\n",
    "        random_state : int, default=42\n",
    "            Random state for reproducibility\n",
    "        n_estimators : int, default=100\n",
    "            Number of trees in the forest\n",
    "        max_depth : int, default=None\n",
    "            Maximum depth of the tree\n",
    "        min_samples_split : int, default=2\n",
    "            Minimum samples required to split a node\n",
    "        min_samples_leaf : int, default=1\n",
    "            Minimum samples required at a leaf node\n",
    "        max_features : str or int, default='sqrt'\n",
    "            Number of features to consider when looking for the best split\n",
    "        class_weight : str or dict, default='balanced'\n",
    "            Weights associated with classes\n",
    "        scale_features : bool, default=True\n",
    "            Whether to scale features\n",
    "        target_col : str, default='cardio'\n",
    "            Name of the target variable column\n",
    "        drop_cols : list, default=None\n",
    "            List of columns to drop (e.g., ID columns)\n",
    "        \"\"\"\n",
    "\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.class_weight = class_weight\n",
    "        self.scale_features = scale_features\n",
    "        self.target_col = target_col\n",
    "        self.drop_cols = drop_cols if drop_cols else []\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.data = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.feature_names = None\n",
    "        self.metrics = {}\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load data from CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Loaded dataframe\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        print(f\"Data loaded successfully: {self.data.shape[0]} rows, {self.data.shape[1]} columns\")\n",
    "        return self.data\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess the data for training.\n",
    "        Uses `self.target_col` and `self.drop_cols` from initialization.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        \n",
    "        df = self.data.copy()\n",
    "        if self.drop_cols:\n",
    "            df = df.drop(columns=self.drop_cols, errors='ignore')\n",
    "        \n",
    "        X = df.drop(columns=[self.target_col])\n",
    "        y = df[self.target_col]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        if self.scale_features:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X_train = pd.DataFrame(\n",
    "                self.scaler.fit_transform(self.X_train),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_train.index\n",
    "            )\n",
    "            self.X_test = pd.DataFrame(\n",
    "                self.scaler.transform(self.X_test),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_test.index\n",
    "            )\n",
    "        \n",
    "        print(f\"Data preprocessed: Train size = {len(self.X_train)}, Test size = {len(self.X_test)}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the Random Forest classifier.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not preprocessed. Call preprocess_data() first.\")\n",
    "        \n",
    "        self.model = RandomForestClassifier(\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            max_features=self.max_features,\n",
    "            class_weight=self.class_weight,\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        print(\"Model trained successfully!\")\n",
    "    \n",
    "    def predict(self, X=None):\n",
    "        \"\"\"\n",
    "        Make predictions on data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame, default=None\n",
    "            Data to predict. If None, uses test set.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        if X is None:\n",
    "            X_to_predict = self.X_test\n",
    "        else:\n",
    "            X_to_predict = X\n",
    "\n",
    "        if X is not None and self.scale_features and self.scaler is not None:\n",
    "            # When predicting on new, external data\n",
    "            X_to_predict = pd.DataFrame(\n",
    "                self.scaler.transform(X_to_predict),\n",
    "                columns=X_to_predict.columns,\n",
    "                index=X_to_predict.index\n",
    "            )\n",
    "        \n",
    "        return self.model.predict(X_to_predict)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test set.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing all performance metrics\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        y_pred = self.predict()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "            'precision': precision_score(self.y_test, y_pred, average='binary'),\n",
    "            'recall': recall_score(self.y_test, y_pred, average='binary'),\n",
    "            'f1_score': f1_score(self.y_test, y_pred, average='binary'),\n",
    "            'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL PERFORMANCE METRICS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Accuracy:  {self.metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {self.metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {self.metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {self.metrics['f1_score']:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(self.metrics['confusion_matrix'])\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred, \n",
    "                                      target_names=['No Disease', 'Disease']))\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def export_predictions(self, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Export predictions for the *entire* loaded dataset to a new CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_filepath : str\n",
    "            Path for the output CSV file\n",
    "        include_all_data : bool, default=True\n",
    "            If True, includes all original columns. If False, only includes\n",
    "            the target and predicted columns.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "            \n",
    "        \n",
    "        # Prepare the full dataset for prediction\n",
    "        X_full = self.data.copy()\n",
    "        \n",
    "        if self.drop_cols:\n",
    "            X_full = X_full.drop(columns=self.drop_cols, errors='ignore')\n",
    "            \n",
    "        X_full = X_full.drop(columns=[self.target_col], errors='ignore')\n",
    "        \n",
    "        # Ensure columns match training data\n",
    "        X_full = X_full[self.feature_names]\n",
    "        \n",
    "        if self.scale_features and self.scaler is not None:\n",
    "            X_full_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X_full),\n",
    "                columns=self.feature_names,\n",
    "                index=X_full.index\n",
    "            )\n",
    "            predictions = self.model.predict(X_full_scaled)\n",
    "        else:\n",
    "            predictions = self.model.predict(X_full)\n",
    "        \n",
    "        if include_all_data:\n",
    "            output_df = self.data.copy()\n",
    "        else:\n",
    "            output_df = pd.DataFrame()\n",
    "            output_df[self.target_col] = self.data[self.target_col]\n",
    "        \n",
    "        output_df['predicted'] = predictions\n",
    "        \n",
    "        output_df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Predictions exported to: {output_filepath}\")\n",
    "        \n",
    "        return output_df\n",
    "    \n",
    "    def run_full_pipeline(self, input_filepath, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline: load, preprocess, train, evaluate, and export.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_filepath : str\n",
    "            Path to input CSV file\n",
    "        output_filepath : str\n",
    "            Path for output CSV file with predictions\n",
    "        include_all_data : bool, default=True\n",
    "            Passed to export_predictions to control output file content.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Starting full pipeline...\\n\")\n",
    "        \n",
    "        self.load_data(input_filepath)\n",
    "        self.preprocess_data()\n",
    "        self.train()\n",
    "        metrics = self.evaluate()\n",
    "        self.export_predictions(output_filepath, include_all_data=include_all_data)\n",
    "        \n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure the model with all parameters during initialization\n",
    "    clf = CardioRandomForestClassifier(\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        min_samples_split=30,\n",
    "        min_samples_leaf=15,\n",
    "        max_features='sqrt',\n",
    "        class_weight='balanced',\n",
    "        scale_features=True,\n",
    "        target_col='cardio',  # <--- Parameter moved here\n",
    "        drop_cols=['id']      # <--- Parameter moved here\n",
    "    )\n",
    "    \n",
    "    # Run the pipeline with just the filepaths\n",
    "    matrix_randomforest = clf.run_full_pipeline(\n",
    "        input_filepath='train.csv',\n",
    "        output_filepath='Prediction_RandomForest.csv' # Changed output name\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84754322",
   "metadata": {},
   "source": [
    "# Naive Base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "573781eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full pipeline...\n",
      "\n",
      "Data loaded successfully: 68520 rows, 13 columns\n",
      "Data preprocessed: Train size = 54816, Test size = 13704\n",
      "Model trained successfully!\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "==================================================\n",
      "Accuracy:  0.7072\n",
      "Precision: 0.7501\n",
      "Recall:    0.6114\n",
      "F1-Score:  0.6737\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5549 1380]\n",
      " [2633 4142]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Disease       0.68      0.80      0.73      6929\n",
      "     Disease       0.75      0.61      0.67      6775\n",
      "\n",
      "    accuracy                           0.71     13704\n",
      "   macro avg       0.71      0.71      0.70     13704\n",
      "weighted avg       0.71      0.71      0.70     13704\n",
      "\n",
      "==================================================\n",
      "\n",
      "Predictions exported to: Prediction_NaiveBayes.csv\n",
      "Pipeline completed successfully!\n",
      "Predictions exported to: Prediction_NaiveBayes.csv\n",
      "Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CardioNaiveBayesClassifier:\n",
    "    \"\"\"\n",
    "    A reusable Naive Bayes classifier for cardiovascular disease prediction.\n",
    "    \n",
    "    Features:\n",
    "    - Train/test split with customizable ratio\n",
    "    - Optional feature scaling\n",
    "    - Comprehensive performance metrics\n",
    "    - Export predictions to CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_size=0.2, random_state=42, var_smoothing=1e-9, \n",
    "                 scale_features=True, target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float, default=0.2\n",
    "            Proportion of dataset to include in test split\n",
    "        random_state : int, default=42\n",
    "            Random state for reproducibility\n",
    "        var_smoothing : float, default=1e-9\n",
    "            Portion of the largest variance of all features that is added to variances for calculation stability\n",
    "        scale_features : bool, default=True\n",
    "            Whether to scale features\n",
    "        target_col : str, default='cardio'\n",
    "            Name of the target variable column\n",
    "        drop_cols : list, default=None\n",
    "            List of columns to drop (e.g., ID columns)\n",
    "        \"\"\"\n",
    "\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.var_smoothing = var_smoothing\n",
    "        self.scale_features = scale_features\n",
    "        self.target_col = target_col\n",
    "        self.drop_cols = drop_cols if drop_cols else []\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.data = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.feature_names = None\n",
    "        self.metrics = {}\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load data from CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Loaded dataframe\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        print(f\"Data loaded successfully: {self.data.shape[0]} rows, {self.data.shape[1]} columns\")\n",
    "        return self.data\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess the data for training.\n",
    "        Uses `self.target_col` and `self.drop_cols` from initialization.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        \n",
    "        df = self.data.copy()\n",
    "        if self.drop_cols:\n",
    "            df = df.drop(columns=self.drop_cols, errors='ignore')\n",
    "        \n",
    "        X = df.drop(columns=[self.target_col])\n",
    "        y = df[self.target_col]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        if self.scale_features:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X_train = pd.DataFrame(\n",
    "                self.scaler.fit_transform(self.X_train),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_train.index\n",
    "            )\n",
    "            self.X_test = pd.DataFrame(\n",
    "                self.scaler.transform(self.X_test),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_test.index\n",
    "            )\n",
    "        \n",
    "        print(f\"Data preprocessed: Train size = {len(self.X_train)}, Test size = {len(self.X_test)}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the Naive Bayes classifier.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not preprocessed. Call preprocess_data() first.\")\n",
    "        \n",
    "        self.model = GaussianNB(\n",
    "            var_smoothing=self.var_smoothing\n",
    "        )\n",
    "        \n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        print(\"Model trained successfully!\")\n",
    "    \n",
    "    def predict(self, X=None):\n",
    "        \"\"\"\n",
    "        Make predictions on data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame, default=None\n",
    "            Data to predict. If None, uses test set.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        if X is None:\n",
    "            X_to_predict = self.X_test\n",
    "        else:\n",
    "            X_to_predict = X\n",
    "\n",
    "        if X is not None and self.scale_features and self.scaler is not None:\n",
    "            # When predicting on new, external data\n",
    "            X_to_predict = pd.DataFrame(\n",
    "                self.scaler.transform(X_to_predict),\n",
    "                columns=X_to_predict.columns,\n",
    "                index=X_to_predict.index\n",
    "            )\n",
    "        \n",
    "        return self.model.predict(X_to_predict)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test set.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing all performance metrics\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        y_pred = self.predict()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "            'precision': precision_score(self.y_test, y_pred, average='binary'),\n",
    "            'recall': recall_score(self.y_test, y_pred, average='binary'),\n",
    "            'f1_score': f1_score(self.y_test, y_pred, average='binary'),\n",
    "            'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL PERFORMANCE METRICS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Accuracy:  {self.metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {self.metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {self.metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {self.metrics['f1_score']:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(self.metrics['confusion_matrix'])\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred, \n",
    "                                      target_names=['No Disease', 'Disease']))\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def export_predictions(self, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Export predictions for the *entire* loaded dataset to a new CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_filepath : str\n",
    "            Path for the output CSV file\n",
    "        include_all_data : bool, default=True\n",
    "            If True, includes all original columns. If False, only includes\n",
    "            the target and predicted columns.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "            \n",
    "        \n",
    "        # Prepare the full dataset for prediction\n",
    "        X_full = self.data.copy()\n",
    "        \n",
    "        if self.drop_cols:\n",
    "            X_full = X_full.drop(columns=self.drop_cols, errors='ignore')\n",
    "            \n",
    "        X_full = X_full.drop(columns=[self.target_col], errors='ignore')\n",
    "        \n",
    "        # Ensure columns match training data\n",
    "        X_full = X_full[self.feature_names]\n",
    "        \n",
    "        if self.scale_features and self.scaler is not None:\n",
    "            X_full_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X_full),\n",
    "                columns=self.feature_names,\n",
    "                index=X_full.index\n",
    "            )\n",
    "            predictions = self.model.predict(X_full_scaled)\n",
    "        else:\n",
    "            predictions = self.model.predict(X_full)\n",
    "        \n",
    "        if include_all_data:\n",
    "            output_df = self.data.copy()\n",
    "        else:\n",
    "            output_df = pd.DataFrame()\n",
    "            output_df[self.target_col] = self.data[self.target_col]\n",
    "        \n",
    "        output_df['predicted'] = predictions\n",
    "        \n",
    "        output_df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Predictions exported to: {output_filepath}\")\n",
    "        \n",
    "        return output_df\n",
    "    \n",
    "    def run_full_pipeline(self, input_filepath, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline: load, preprocess, train, evaluate, and export.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_filepath : str\n",
    "            Path to input CSV file\n",
    "        output_filepath : str\n",
    "            Path for output CSV file with predictions\n",
    "        include_all_data : bool, default=True\n",
    "            Passed to export_predictions to control output file content.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Starting full pipeline...\\n\")\n",
    "        \n",
    "        self.load_data(input_filepath)\n",
    "        self.preprocess_data()\n",
    "        self.train()\n",
    "        metrics = self.evaluate()\n",
    "        self.export_predictions(output_filepath, include_all_data=include_all_data)\n",
    "        \n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure the model with all parameters during initialization\n",
    "    clf = CardioNaiveBayesClassifier(\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        var_smoothing=1e-8,\n",
    "        scale_features=True,\n",
    "        target_col='cardio',  # <--- Parameter moved here\n",
    "        drop_cols=['id']      # <--- Parameter moved here\n",
    "    )\n",
    "    \n",
    "    # Run the pipeline with just the filepaths\n",
    "    matrix_naivebayes = clf.run_full_pipeline(\n",
    "        input_filepath='train.csv',\n",
    "        output_filepath='Prediction_NaiveBayes.csv' # Changed output name\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6342e2",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3eb750b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full pipeline...\n",
      "\n",
      "Data loaded successfully: 68520 rows, 13 columns\n",
      "Data preprocessed: Train size = 54816, Test size = 13704\n",
      "Model trained successfully!\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "==================================================\n",
      "Accuracy:  0.7330\n",
      "Precision: 0.7514\n",
      "Recall:    0.6874\n",
      "F1-Score:  0.7180\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5388 1541]\n",
      " [2118 4657]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Disease       0.72      0.78      0.75      6929\n",
      "     Disease       0.75      0.69      0.72      6775\n",
      "\n",
      "    accuracy                           0.73     13704\n",
      "   macro avg       0.73      0.73      0.73     13704\n",
      "weighted avg       0.73      0.73      0.73     13704\n",
      "\n",
      "==================================================\n",
      "\n",
      "Model trained successfully!\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "==================================================\n",
      "Accuracy:  0.7330\n",
      "Precision: 0.7514\n",
      "Recall:    0.6874\n",
      "F1-Score:  0.7180\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5388 1541]\n",
      " [2118 4657]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Disease       0.72      0.78      0.75      6929\n",
      "     Disease       0.75      0.69      0.72      6775\n",
      "\n",
      "    accuracy                           0.73     13704\n",
      "   macro avg       0.73      0.73      0.73     13704\n",
      "weighted avg       0.73      0.73      0.73     13704\n",
      "\n",
      "==================================================\n",
      "\n",
      "Predictions exported to: Prediction_GradientBoosting.csv\n",
      "Pipeline completed successfully!\n",
      "Predictions exported to: Prediction_GradientBoosting.csv\n",
      "Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CardioGradientBoostingClassifier:\n",
    "    \"\"\"\n",
    "    A reusable Gradient Boosting classifier for cardiovascular disease prediction.\n",
    "    \n",
    "    Features:\n",
    "    - Train/test split with customizable ratio\n",
    "    - Optional feature scaling\n",
    "    - Comprehensive performance metrics\n",
    "    - Export predictions to CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_size=0.2, random_state=42, n_estimators=100, \n",
    "                 learning_rate=0.1, max_depth=3, subsample=1.0,\n",
    "                 scale_features=True, target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float, default=0.2\n",
    "            Proportion of dataset to include in test split\n",
    "        random_state : int, default=42\n",
    "            Random state for reproducibility\n",
    "        n_estimators : int, default=100\n",
    "            The number of boosting stages to perform\n",
    "        learning_rate : float, default=0.1\n",
    "            Learning rate shrinks the contribution of each tree by learning_rate\n",
    "        max_depth : int, default=3\n",
    "            Maximum depth of the individual regression estimators\n",
    "        subsample : float, default=1.0\n",
    "            The fraction of samples to be used for fitting the individual base learners\n",
    "        scale_features : bool, default=True\n",
    "            Whether to scale features\n",
    "        target_col : str, default='cardio'\n",
    "            Name of the target variable column\n",
    "        drop_cols : list, default=None\n",
    "            List of columns to drop (e.g., ID columns)\n",
    "        \"\"\"\n",
    "\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.subsample = subsample\n",
    "        self.scale_features = scale_features\n",
    "        self.target_col = target_col\n",
    "        self.drop_cols = drop_cols if drop_cols else []\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.data = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.feature_names = None\n",
    "        self.metrics = {}\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load data from CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Loaded dataframe\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        print(f\"Data loaded successfully: {self.data.shape[0]} rows, {self.data.shape[1]} columns\")\n",
    "        return self.data\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess the data for training.\n",
    "        Uses `self.target_col` and `self.drop_cols` from initialization.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        \n",
    "        df = self.data.copy()\n",
    "        if self.drop_cols:\n",
    "            df = df.drop(columns=self.drop_cols, errors='ignore')\n",
    "        \n",
    "        X = df.drop(columns=[self.target_col])\n",
    "        y = df[self.target_col]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        if self.scale_features:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X_train = pd.DataFrame(\n",
    "                self.scaler.fit_transform(self.X_train),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_train.index\n",
    "            )\n",
    "            self.X_test = pd.DataFrame(\n",
    "                self.scaler.transform(self.X_test),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_test.index\n",
    "            )\n",
    "        \n",
    "        print(f\"Data preprocessed: Train size = {len(self.X_train)}, Test size = {len(self.X_test)}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the Gradient Boosting classifier.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not preprocessed. Call preprocess_data() first.\")\n",
    "        \n",
    "        self.model = GradientBoostingClassifier(\n",
    "            n_estimators=self.n_estimators,\n",
    "            learning_rate=self.learning_rate,\n",
    "            max_depth=self.max_depth,\n",
    "            subsample=self.subsample,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        print(\"Model trained successfully!\")\n",
    "    \n",
    "    def predict(self, X=None):\n",
    "        \"\"\"\n",
    "        Make predictions on data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame, default=None\n",
    "            Data to predict. If None, uses test set.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        if X is None:\n",
    "            X_to_predict = self.X_test\n",
    "        else:\n",
    "            X_to_predict = X\n",
    "\n",
    "        if X is not None and self.scale_features and self.scaler is not None:\n",
    "            # When predicting on new, external data\n",
    "            X_to_predict = pd.DataFrame(\n",
    "                self.scaler.transform(X_to_predict),\n",
    "                columns=X_to_predict.columns,\n",
    "                index=X_to_predict.index\n",
    "            )\n",
    "        \n",
    "        return self.model.predict(X_to_predict)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test set.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing all performance metrics\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        y_pred = self.predict()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "            'precision': precision_score(self.y_test, y_pred, average='binary'),\n",
    "            'recall': recall_score(self.y_test, y_pred, average='binary'),\n",
    "            'f1_score': f1_score(self.y_test, y_pred, average='binary'),\n",
    "            'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL PERFORMANCE METRICS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Accuracy:  {self.metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {self.metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {self.metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {self.metrics['f1_score']:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(self.metrics['confusion_matrix'])\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred, \n",
    "                                      target_names=['No Disease', 'Disease']))\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def export_predictions(self, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Export predictions for the *entire* loaded dataset to a new CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_filepath : str\n",
    "            Path for the output CSV file\n",
    "        include_all_data : bool, default=True\n",
    "            If True, includes all original columns. If False, only includes\n",
    "            the target and predicted columns.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "            \n",
    "        \n",
    "        # Prepare the full dataset for prediction\n",
    "        X_full = self.data.copy()\n",
    "        \n",
    "        if self.drop_cols:\n",
    "            X_full = X_full.drop(columns=self.drop_cols, errors='ignore')\n",
    "            \n",
    "        X_full = X_full.drop(columns=[self.target_col], errors='ignore')\n",
    "        \n",
    "        # Ensure columns match training data\n",
    "        X_full = X_full[self.feature_names]\n",
    "        \n",
    "        if self.scale_features and self.scaler is not None:\n",
    "            X_full_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X_full),\n",
    "                columns=self.feature_names,\n",
    "                index=X_full.index\n",
    "            )\n",
    "            predictions = self.model.predict(X_full_scaled)\n",
    "        else:\n",
    "            predictions = self.model.predict(X_full)\n",
    "        \n",
    "        if include_all_data:\n",
    "            output_df = self.data.copy()\n",
    "        else:\n",
    "            output_df = pd.DataFrame()\n",
    "            output_df[self.target_col] = self.data[self.target_col]\n",
    "        \n",
    "        output_df['predicted'] = predictions\n",
    "        \n",
    "        output_df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Predictions exported to: {output_filepath}\")\n",
    "        \n",
    "        return output_df\n",
    "    \n",
    "    def run_full_pipeline(self, input_filepath, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline: load, preprocess, train, evaluate, and export.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_filepath : str\n",
    "            Path to input CSV file\n",
    "        output_filepath : str\n",
    "            Path for output CSV file with predictions\n",
    "        include_all_data : bool, default=True\n",
    "            Passed to export_predictions to control output file content.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Starting full pipeline...\\n\")\n",
    "        \n",
    "        self.load_data(input_filepath)\n",
    "        self.preprocess_data()\n",
    "        self.train()\n",
    "        metrics = self.evaluate()\n",
    "        self.export_predictions(output_filepath, include_all_data=include_all_data)\n",
    "        \n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure the model with all parameters during initialization\n",
    "    clf = CardioGradientBoostingClassifier(\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        scale_features=True,\n",
    "        target_col='cardio',  # <--- Parameter moved here\n",
    "        drop_cols=['id']      # <--- Parameter moved here\n",
    "    )\n",
    "    \n",
    "    # Run the pipeline with just the filepaths\n",
    "    matrix_gradientboosting = clf.run_full_pipeline(\n",
    "        input_filepath='train.csv',\n",
    "        output_filepath='Prediction_GradientBoosting.csv' # Changed output name\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436d4908",
   "metadata": {},
   "source": [
    "# SVM \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77ba8a4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CardioSupportVectorClassifier.__init__() got an unexpected keyword argument 'class_weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 289\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    288\u001b[39m     \u001b[38;5;66;03m# Configure the model with all parameters during initialization\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     clf = \u001b[43mCardioSupportVectorClassifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[43m        \u001b[49m\u001b[43mC\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrbf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbalanced\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscale_features\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcardio\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# <--- Parameter moved here\u001b[39;49;00m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdrop_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# <--- Parameter moved here\u001b[39;49;00m\n\u001b[32m    299\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m     \u001b[38;5;66;03m# Run the pipeline with just the filepaths\u001b[39;00m\n\u001b[32m    302\u001b[39m     matrix_svm = clf.run_full_pipeline(\n\u001b[32m    303\u001b[39m         input_filepath=\u001b[33m'\u001b[39m\u001b[33mtrain.csv\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    304\u001b[39m         output_filepath=\u001b[33m'\u001b[39m\u001b[33mPrediction_SVM.csv\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;66;03m# Changed output name\u001b[39;00m\n\u001b[32m    305\u001b[39m     )\n",
      "\u001b[31mTypeError\u001b[39m: CardioSupportVectorClassifier.__init__() got an unexpected keyword argument 'class_weight'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CardioSupportVectorClassifier:\n",
    "    \"\"\"\n",
    "    A reusable Support Vector Machine classifier for cardiovascular disease prediction.\n",
    "    \n",
    "    Features:\n",
    "    - Train/test split with customizable ratio\n",
    "    - Optional feature scaling\n",
    "    - Comprehensive performance metrics\n",
    "    - Export predictions to CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_size=0.2, random_state=42, C=1.0, \n",
    "                 kernel='rbf', gamma='scale', scale_features=True,\n",
    "                 target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float, default=0.2\n",
    "            Proportion of dataset to include in test split\n",
    "        random_state : int, default=42\n",
    "            Random state for reproducibility\n",
    "        C : float, default=1.0\n",
    "            Regularization parameter. The strength of the regularization is inversely proportional to C\n",
    "        kernel : str, default='rbf'\n",
    "            Specifies the kernel type to be used in the algorithm\n",
    "        gamma : float or str, default='scale'\n",
    "            Kernel coefficient for rbf, poly and sigmoid\n",
    "        scale_features : bool, default=True\n",
    "            Whether to scale features\n",
    "        target_col : str, default='cardio'\n",
    "            Name of the target variable column\n",
    "        drop_cols : list, default=None\n",
    "            List of columns to drop (e.g., ID columns)\n",
    "        \"\"\"\n",
    "\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.gamma = gamma\n",
    "        self.scale_features = scale_features\n",
    "        self.target_col = target_col\n",
    "        self.drop_cols = drop_cols if drop_cols else []\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.data = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.feature_names = None\n",
    "        self.metrics = {}\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load data from CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Loaded dataframe\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        print(f\"Data loaded successfully: {self.data.shape[0]} rows, {self.data.shape[1]} columns\")\n",
    "        return self.data\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess the data for training.\n",
    "        Uses `self.target_col` and `self.drop_cols` from initialization.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        \n",
    "        df = self.data.copy()\n",
    "        if self.drop_cols:\n",
    "            df = df.drop(columns=self.drop_cols, errors='ignore')\n",
    "        \n",
    "        X = df.drop(columns=[self.target_col])\n",
    "        y = df[self.target_col]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        if self.scale_features:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X_train = pd.DataFrame(\n",
    "                self.scaler.fit_transform(self.X_train),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_train.index\n",
    "            )\n",
    "            self.X_test = pd.DataFrame(\n",
    "                self.scaler.transform(self.X_test),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_test.index\n",
    "            )\n",
    "        \n",
    "        print(f\"Data preprocessed: Train size = {len(self.X_train)}, Test size = {len(self.X_test)}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the Support Vector Machine classifier.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not preprocessed. Call preprocess_data() first.\")\n",
    "        \n",
    "        self.model = SVC(\n",
    "            C=self.C,\n",
    "            kernel=self.kernel,\n",
    "            gamma=self.gamma,\n",
    "            class_weight=self.class_weight,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        \n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        print(\"Model trained successfully!\")\n",
    "    \n",
    "    def predict(self, X=None):\n",
    "        \"\"\"\n",
    "        Make predictions on data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame, default=None\n",
    "            Data to predict. If None, uses test set.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        if X is None:\n",
    "            X_to_predict = self.X_test\n",
    "        else:\n",
    "            X_to_predict = X\n",
    "\n",
    "        if X is not None and self.scale_features and self.scaler is not None:\n",
    "            # When predicting on new, external data\n",
    "            X_to_predict = pd.DataFrame(\n",
    "                self.scaler.transform(X_to_predict),\n",
    "                columns=X_to_predict.columns,\n",
    "                index=X_to_predict.index\n",
    "            )\n",
    "        \n",
    "        return self.model.predict(X_to_predict)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test set.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing all performance metrics\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        y_pred = self.predict()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "            'precision': precision_score(self.y_test, y_pred, average='binary'),\n",
    "            'recall': recall_score(self.y_test, y_pred, average='binary'),\n",
    "            'f1_score': f1_score(self.y_test, y_pred, average='binary'),\n",
    "            'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL PERFORMANCE METRICS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Accuracy:  {self.metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {self.metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {self.metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {self.metrics['f1_score']:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(self.metrics['confusion_matrix'])\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred, \n",
    "                                      target_names=['No Disease', 'Disease']))\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def export_predictions(self, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Export predictions for the *entire* loaded dataset to a new CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_filepath : str\n",
    "            Path for the output CSV file\n",
    "        include_all_data : bool, default=True\n",
    "            If True, includes all original columns. If False, only includes\n",
    "            the target and predicted columns.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "            \n",
    "        \n",
    "        # Prepare the full dataset for prediction\n",
    "        X_full = self.data.copy()\n",
    "        \n",
    "        if self.drop_cols:\n",
    "            X_full = X_full.drop(columns=self.drop_cols, errors='ignore')\n",
    "            \n",
    "        X_full = X_full.drop(columns=[self.target_col], errors='ignore')\n",
    "        \n",
    "        # Ensure columns match training data\n",
    "        X_full = X_full[self.feature_names]\n",
    "        \n",
    "        if self.scale_features and self.scaler is not None:\n",
    "            X_full_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X_full),\n",
    "                columns=self.feature_names,\n",
    "                index=X_full.index\n",
    "            )\n",
    "            predictions = self.model.predict(X_full_scaled)\n",
    "        else:\n",
    "            predictions = self.model.predict(X_full)\n",
    "        \n",
    "        if include_all_data:\n",
    "            output_df = self.data.copy()\n",
    "        else:\n",
    "            output_df = pd.DataFrame()\n",
    "            output_df[self.target_col] = self.data[self.target_col]\n",
    "        \n",
    "        output_df['predicted'] = predictions\n",
    "        \n",
    "        output_df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Predictions exported to: {output_filepath}\")\n",
    "        \n",
    "        return output_df\n",
    "    \n",
    "    def run_full_pipeline(self, input_filepath, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline: load, preprocess, train, evaluate, and export.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_filepath : str\n",
    "            Path to input CSV file\n",
    "        output_filepath : str\n",
    "            Path for output CSV file with predictions\n",
    "        include_all_data : bool, default=True\n",
    "            Passed to export_predictions to control output file content.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Starting full pipeline...\\n\")\n",
    "        \n",
    "        self.load_data(input_filepath)\n",
    "        self.preprocess_data()\n",
    "        self.train()\n",
    "        metrics = self.evaluate()\n",
    "        self.export_predictions(output_filepath, include_all_data=include_all_data)\n",
    "        \n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure the model with all parameters during initialization\n",
    "    clf = CardioSupportVectorClassifier(\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        C=10.0,\n",
    "        kernel='rbf',\n",
    "        gamma='auto',\n",
    "        class_weight='balanced',\n",
    "        scale_features=True,\n",
    "        target_col='cardio',  # <--- Parameter moved here\n",
    "        drop_cols=['id']      # <--- Parameter moved here\n",
    "    )\n",
    "    \n",
    "    # Run the pipeline with just the filepaths\n",
    "    matrix_svm = clf.run_full_pipeline(\n",
    "        input_filepath='train.csv',\n",
    "        output_filepath='Prediction_SVM.csv' # Changed output name\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c68c01",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c9fd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting full pipeline...\n",
      "\n",
      "Data loaded successfully: 68520 rows, 13 columns\n",
      "Data preprocessed: Train size = 54816, Test size = 13704\n",
      "Model trained successfully!\n",
      "Model trained successfully!\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "==================================================\n",
      "Accuracy:  0.6697\n",
      "Precision: 0.6699\n",
      "Recall:    0.6545\n",
      "F1-Score:  0.6621\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4744 2185]\n",
      " [2341 4434]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Disease       0.67      0.68      0.68      6929\n",
      "     Disease       0.67      0.65      0.66      6775\n",
      "\n",
      "    accuracy                           0.67     13704\n",
      "   macro avg       0.67      0.67      0.67     13704\n",
      "weighted avg       0.67      0.67      0.67     13704\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "==================================================\n",
      "Accuracy:  0.6697\n",
      "Precision: 0.6699\n",
      "Recall:    0.6545\n",
      "F1-Score:  0.6621\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4744 2185]\n",
      " [2341 4434]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Disease       0.67      0.68      0.68      6929\n",
      "     Disease       0.67      0.65      0.66      6775\n",
      "\n",
      "    accuracy                           0.67     13704\n",
      "   macro avg       0.67      0.67      0.67     13704\n",
      "weighted avg       0.67      0.67      0.67     13704\n",
      "\n",
      "==================================================\n",
      "\n",
      "Predictions exported to: Prediction_KNN.csv\n",
      "Pipeline completed successfully!\n",
      "Predictions exported to: Prediction_KNN.csv\n",
      "Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CardioKNeighborsClassifier:\n",
    "    \"\"\"\n",
    "    A reusable K-Nearest Neighbors classifier for cardiovascular disease prediction.\n",
    "    \n",
    "    Features:\n",
    "    - Train/test split with customizable ratio\n",
    "    - Optional feature scaling\n",
    "    - Comprehensive performance metrics\n",
    "    - Export predictions to CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_size=0.2, random_state=42, n_neighbors=5, \n",
    "                 weights='uniform', metric='euclidean', scale_features=True,\n",
    "                 target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float, default=0.2\n",
    "            Proportion of dataset to include in test split\n",
    "        random_state : int, default=42\n",
    "            Random state for reproducibility\n",
    "        n_neighbors : int, default=5\n",
    "            Number of neighbors to use by default for kneighbors queries\n",
    "        weights : str, default='uniform'\n",
    "            Weight function used in prediction\n",
    "        metric : str, default='euclidean'\n",
    "            Distance metric to use for the tree\n",
    "        scale_features : bool, default=True\n",
    "            Whether to scale features\n",
    "        target_col : str, default='cardio'\n",
    "            Name of the target variable column\n",
    "        drop_cols : list, default=None\n",
    "            List of columns to drop (e.g., ID columns)\n",
    "        \"\"\"\n",
    "\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.weights = weights\n",
    "        self.metric = metric\n",
    "        self.scale_features = scale_features\n",
    "        self.target_col = target_col\n",
    "        self.drop_cols = drop_cols if drop_cols else []\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.data = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.feature_names = None\n",
    "        self.metrics = {}\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load data from CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Loaded dataframe\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        print(f\"Data loaded successfully: {self.data.shape[0]} rows, {self.data.shape[1]} columns\")\n",
    "        return self.data\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess the data for training.\n",
    "        Uses `self.target_col` and `self.drop_cols` from initialization.\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        \n",
    "        df = self.data.copy()\n",
    "        if self.drop_cols:\n",
    "            df = df.drop(columns=self.drop_cols, errors='ignore')\n",
    "        \n",
    "        X = df.drop(columns=[self.target_col])\n",
    "        y = df[self.target_col]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        if self.scale_features:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X_train = pd.DataFrame(\n",
    "                self.scaler.fit_transform(self.X_train),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_train.index\n",
    "            )\n",
    "            self.X_test = pd.DataFrame(\n",
    "                self.scaler.transform(self.X_test),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_test.index\n",
    "            )\n",
    "        \n",
    "        print(f\"Data preprocessed: Train size = {len(self.X_train)}, Test size = {len(self.X_test)}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the K-Nearest Neighbors classifier.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not preprocessed. Call preprocess_data() first.\")\n",
    "        \n",
    "        self.model = KNeighborsClassifier(\n",
    "            n_neighbors=self.n_neighbors,\n",
    "            weights=self.weights,\n",
    "            metric=self.metric\n",
    "        )\n",
    "        \n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        print(\"Model trained successfully!\")\n",
    "    \n",
    "    def predict(self, X=None):\n",
    "        \"\"\"\n",
    "        Make predictions on data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame, default=None\n",
    "            Data to predict. If None, uses test set.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        if X is None:\n",
    "            X_to_predict = self.X_test\n",
    "        else:\n",
    "            X_to_predict = X\n",
    "\n",
    "        if X is not None and self.scale_features and self.scaler is not None:\n",
    "            # When predicting on new, external data\n",
    "            X_to_predict = pd.DataFrame(\n",
    "                self.scaler.transform(X_to_predict),\n",
    "                columns=X_to_predict.columns,\n",
    "                index=X_to_predict.index\n",
    "            )\n",
    "        \n",
    "        return self.model.predict(X_to_predict)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test set.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing all performance metrics\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        y_pred = self.predict()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "            'precision': precision_score(self.y_test, y_pred, average='binary'),\n",
    "            'recall': recall_score(self.y_test, y_pred, average='binary'),\n",
    "            'f1_score': f1_score(self.y_test, y_pred, average='binary'),\n",
    "            'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL PERFORMANCE METRICS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Accuracy:  {self.metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {self.metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {self.metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {self.metrics['f1_score']:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(self.metrics['confusion_matrix'])\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred, \n",
    "                                      target_names=['No Disease', 'Disease']))\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def export_predictions(self, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Export predictions for the *entire* loaded dataset to a new CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_filepath : str\n",
    "            Path for the output CSV file\n",
    "        include_all_data : bool, default=True\n",
    "            If True, includes all original columns. If False, only includes\n",
    "            the target and predicted columns.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "            \n",
    "        \n",
    "        # Prepare the full dataset for prediction\n",
    "        X_full = self.data.copy()\n",
    "        \n",
    "        if self.drop_cols:\n",
    "            X_full = X_full.drop(columns=self.drop_cols, errors='ignore')\n",
    "            \n",
    "        X_full = X_full.drop(columns=[self.target_col], errors='ignore')\n",
    "        \n",
    "        # Ensure columns match training data\n",
    "        X_full = X_full[self.feature_names]\n",
    "        \n",
    "        if self.scale_features and self.scaler is not None:\n",
    "            X_full_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X_full),\n",
    "                columns=self.feature_names,\n",
    "                index=X_full.index\n",
    "            )\n",
    "            predictions = self.model.predict(X_full_scaled)\n",
    "        else:\n",
    "            predictions = self.model.predict(X_full)\n",
    "        \n",
    "        if include_all_data:\n",
    "            output_df = self.data.copy()\n",
    "        else:\n",
    "            output_df = pd.DataFrame()\n",
    "            output_df[self.target_col] = self.data[self.target_col]\n",
    "        \n",
    "        output_df['predicted'] = predictions\n",
    "        \n",
    "        output_df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Predictions exported to: {output_filepath}\")\n",
    "        \n",
    "        return output_df\n",
    "    \n",
    "    def run_full_pipeline(self, input_filepath, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline: load, preprocess, train, evaluate, and export.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_filepath : str\n",
    "            Path to input CSV file\n",
    "        output_filepath : str\n",
    "            Path for output CSV file with predictions\n",
    "        include_all_data : bool, default=True\n",
    "            Passed to export_predictions to control output file content.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Starting full pipeline...\\n\")\n",
    "        \n",
    "        self.load_data(input_filepath)\n",
    "        self.preprocess_data()\n",
    "        self.train()\n",
    "        metrics = self.evaluate()\n",
    "        self.export_predictions(output_filepath, include_all_data=include_all_data)\n",
    "        \n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure the model with all parameters during initialization\n",
    "    clf = CardioKNeighborsClassifier(\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        n_neighbors=15,\n",
    "        weights='distance',\n",
    "        metric='minkowski',\n",
    "        scale_features=True,\n",
    "        target_col='cardio',  # <--- Parameter moved here\n",
    "        drop_cols=['id']      # <--- Parameter moved here\n",
    "    )\n",
    "    \n",
    "    # Run the pipeline with just the filepaths\n",
    "    matrix_knn = clf.run_full_pipeline(\n",
    "        input_filepath='train.csv',\n",
    "        output_filepath='Prediction_KNN.csv' # Changed output name\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b771e70",
   "metadata": {},
   "source": [
    "# DL Algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9473896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\users\\xor\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: torchvision in c:\\users\\xor\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.24.0+cu126)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Running MLP Pipeline...\n",
      "Starting full pipeline...\n",
      "\n",
      "Data loaded successfully: 68520 rows, 13 columns\n",
      "Data preprocessed: Train size = 54816, Test size = 13704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torchaudio (from versions: none)\n",
      "ERROR: No matching distribution found for torchaudio\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100], Loss: 0.5279\n",
      "Epoch [100/100], Loss: 0.5191\n",
      "Model trained successfully!\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "==================================================\n",
      "Accuracy:  0.7221\n",
      "Precision: 0.7469\n",
      "Recall:    0.6624\n",
      "F1-Score:  0.7021\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5408 1521]\n",
      " [2287 4488]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Disease       0.70      0.78      0.74      6929\n",
      "     Disease       0.75      0.66      0.70      6775\n",
      "\n",
      "    accuracy                           0.72     13704\n",
      "   macro avg       0.72      0.72      0.72     13704\n",
      "weighted avg       0.72      0.72      0.72     13704\n",
      "\n",
      "==================================================\n",
      "\n",
      "Epoch [100/100], Loss: 0.5191\n",
      "Model trained successfully!\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE METRICS\n",
      "==================================================\n",
      "Accuracy:  0.7221\n",
      "Precision: 0.7469\n",
      "Recall:    0.6624\n",
      "F1-Score:  0.7021\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5408 1521]\n",
      " [2287 4488]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Disease       0.70      0.78      0.74      6929\n",
      "     Disease       0.75      0.66      0.70      6775\n",
      "\n",
      "    accuracy                           0.72     13704\n",
      "   macro avg       0.72      0.72      0.72     13704\n",
      "weighted avg       0.72      0.72      0.72     13704\n",
      "\n",
      "==================================================\n",
      "\n",
      "Predictions exported to: Prediction_MLP.csv\n",
      "Pipeline completed successfully!\n",
      "Predictions exported to: Prediction_MLP.csv\n",
      "Pipeline completed successfully!\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class CardioMLPClassifier:\n",
    "    \"\"\"\n",
    "    A reusable Multi-Layer Perceptron (MLP) deep learning classifier for cardiovascular disease prediction using PyTorch.\n",
    "    \n",
    "    Features:\n",
    "    - Train/test split with customizable ratio\n",
    "    - Optional feature scaling\n",
    "    - Comprehensive performance metrics\n",
    "    - Export predictions to CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_size=0.2, random_state=42, hidden_sizes=[64, 32], \n",
    "                 lr=0.001, epochs=100, batch_size=32, scale_features=True):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float, default=0.2\n",
    "            Proportion of dataset to include in test split\n",
    "        random_state : int, default=42\n",
    "            Random state for reproducibility\n",
    "        hidden_sizes : list, default=[64, 32]\n",
    "            List of hidden layer sizes\n",
    "        lr : float, default=0.001\n",
    "            Learning rate for the optimizer\n",
    "        epochs : int, default=100\n",
    "            Number of training epochs\n",
    "        batch_size : int, default=32\n",
    "            Batch size for training\n",
    "        scale_features : bool, default=True\n",
    "            Whether to scale features\n",
    "        \"\"\"\n",
    "\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.scale_features = scale_features\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.X_train_tensor = None\n",
    "        self.y_train_tensor = None\n",
    "        self.feature_names = None\n",
    "        self.metrics = {}\n",
    "        self.drop_cols = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load data from CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Loaded dataframe\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        print(f\"Data loaded successfully: {self.data.shape[0]} rows, {self.data.shape[1]} columns\")\n",
    "        return self.data\n",
    "    \n",
    "    def preprocess_data(self, target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Preprocess the data for training.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        target_col : str, default='cardio'\n",
    "            Name of the target variable column\n",
    "        drop_cols : list, default=None\n",
    "            List of columns to drop (e.g., ID columns)\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        \n",
    "        self.drop_cols = drop_cols if drop_cols else []\n",
    "        \n",
    "        df = self.data.copy()\n",
    "        if drop_cols:\n",
    "            df = df.drop(columns=drop_cols, errors='ignore')\n",
    "        \n",
    "        X = df.drop(columns=[target_col])\n",
    "        y = df[target_col]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        if self.scale_features:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X_train = pd.DataFrame(\n",
    "                self.scaler.fit_transform(self.X_train),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_train.index\n",
    "            )\n",
    "            self.X_test = pd.DataFrame(\n",
    "                self.scaler.transform(self.X_test),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_test.index\n",
    "            )\n",
    "        \n",
    "        print(f\"Data preprocessed: Train size = {len(self.X_train)}, Test size = {len(self.X_test)}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the MLP classifier.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not preprocessed. Call preprocess_data() first.\")\n",
    "        \n",
    "        # Convert to tensors\n",
    "        X_train_tensor = torch.FloatTensor(self.X_train.values).to(self.device)\n",
    "        y_train_tensor = torch.LongTensor(self.y_train.values).to(self.device)\n",
    "        \n",
    "        self.X_train_tensor = X_train_tensor\n",
    "        self.y_train_tensor = y_train_tensor\n",
    "        \n",
    "        # Define model\n",
    "        input_size = len(self.feature_names)\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        for size in self.hidden_sizes:\n",
    "            layers.append(nn.Linear(prev_size, size))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = size\n",
    "        layers.append(nn.Linear(prev_size, 2))  # Binary classification\n",
    "        \n",
    "        self.model = nn.Sequential(*layers).to(self.device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        # DataLoader\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            running_loss = 0.0\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                avg_loss = running_loss / len(train_loader)\n",
    "                print(f'Epoch [{epoch+1}/{self.epochs}], Loss: {avg_loss:.4f}')\n",
    "        \n",
    "        print(\"Model trained successfully!\")\n",
    "    \n",
    "    def predict(self, X=None):\n",
    "        \"\"\"\n",
    "        Make predictions on data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame, default=None\n",
    "            Data to predict. If None, uses test set.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            if X is None:\n",
    "                input_tensor = torch.FloatTensor(self.X_test.values).to(self.device)\n",
    "            else:\n",
    "                if self.scale_features and self.scaler is not None:\n",
    "                    X_scaled = pd.DataFrame(\n",
    "                        self.scaler.transform(X),\n",
    "                        columns=X.columns,\n",
    "                        index=X.index\n",
    "                    )\n",
    "                    input_tensor = torch.FloatTensor(X_scaled.values).to(self.device)\n",
    "                else:\n",
    "                    input_tensor = torch.FloatTensor(X.values).to(self.device)\n",
    "            \n",
    "            outputs = self.model(input_tensor)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            return predicted.cpu().numpy()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test set.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing all performance metrics\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        y_pred = self.predict()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "            'precision': precision_score(self.y_test, y_pred, average='binary'),\n",
    "            'recall': recall_score(self.y_test, y_pred, average='binary'),\n",
    "            'f1_score': f1_score(self.y_test, y_pred, average='binary'),\n",
    "            'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL PERFORMANCE METRICS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Accuracy:  {self.metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {self.metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {self.metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {self.metrics['f1_score']:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(self.metrics['confusion_matrix'])\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred, \n",
    "                                   target_names=['No Disease', 'Disease']))\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def export_predictions(self, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Export predictions to a new CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_filepath : str\n",
    "            Path for the output CSV file\n",
    "        include_all_data : bool, default=True\n",
    "            If True, includes all original columns. If False, only includes\n",
    "            the target and predicted columns.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        X_full = self.data.copy()\n",
    "        \n",
    "        if self.drop_cols:\n",
    "            X_full = X_full.drop(columns=self.drop_cols, errors='ignore')\n",
    "        \n",
    "        X_full = X_full.drop(columns=['cardio'], errors='ignore')\n",
    "        \n",
    "        if self.scale_features and self.scaler is not None:\n",
    "            X_full_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X_full),\n",
    "                columns=self.feature_names,\n",
    "                index=X_full.index\n",
    "            )\n",
    "            predictions = self.predict(X_full_scaled)\n",
    "        else:\n",
    "            predictions = self.predict(X_full)\n",
    "        \n",
    "        if include_all_data:\n",
    "            output_df = self.data.copy()\n",
    "        else:\n",
    "            output_df = pd.DataFrame()\n",
    "            output_df['cardio'] = self.data['cardio']\n",
    "        \n",
    "        output_df['predicted'] = predictions\n",
    "        \n",
    "        output_df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Predictions exported to: {output_filepath}\")\n",
    "        \n",
    "        return output_df\n",
    "    \n",
    "    def run_full_pipeline(self, input_filepath, output_filepath, \n",
    "                         target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline: load, preprocess, train, evaluate, and export.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_filepath : str\n",
    "            Path to input CSV file\n",
    "        output_filepath : str\n",
    "            Path for output CSV file with predictions\n",
    "        target_col : str, default='cardio'\n",
    "            Name of target variable\n",
    "        drop_cols : list, default=None\n",
    "            Columns to drop before training\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Starting full pipeline...\\n\")\n",
    "        \n",
    "        self.load_data(input_filepath)\n",
    "        self.preprocess_data(target_col=target_col, drop_cols=drop_cols)\n",
    "        self.train()\n",
    "        metrics = self.evaluate()\n",
    "        self.export_predictions(output_filepath)\n",
    "        \n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "class CardioTabularCNNClassifier:\n",
    "    \"\"\"\n",
    "    A reusable 1D Convolutional Neural Network (CNN) classifier for tabular cardiovascular disease prediction using PyTorch.\n",
    "    Treats features as a 1D sequence.\n",
    "    \n",
    "    Features:\n",
    "    - Train/test split with customizable ratio\n",
    "    - Optional feature scaling\n",
    "    - Comprehensive performance metrics\n",
    "    - Export predictions to CSV\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, test_size=0.2, random_state=42, filters=[32, 64], \n",
    "                 kernel_sizes=[3, 3], lr=0.001, epochs=100, batch_size=32, \n",
    "                 scale_features=True):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float, default=0.2\n",
    "            Proportion of dataset to include in test split\n",
    "        random_state : int, default=42\n",
    "            Random state for reproducibility\n",
    "        filters : list, default=[32, 64]\n",
    "            List of number of filters for each conv layer\n",
    "        kernel_sizes : list, default=[3, 3]\n",
    "            List of kernel sizes for each conv layer\n",
    "        lr : float, default=0.001\n",
    "            Learning rate for the optimizer\n",
    "        epochs : int, default=100\n",
    "            Number of training epochs\n",
    "        batch_size : int, default=32\n",
    "            Batch size for training\n",
    "        scale_features : bool, default=True\n",
    "            Whether to scale features\n",
    "        \"\"\"\n",
    "\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        self.filters = filters\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.scale_features = scale_features\n",
    "        \n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.X_train_tensor = None\n",
    "        self.y_train_tensor = None\n",
    "        self.feature_names = None\n",
    "        self.metrics = {}\n",
    "        self.drop_cols = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def load_data(self, filepath):\n",
    "        \"\"\"\n",
    "        Load data from CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        filepath : str\n",
    "            Path to the CSV file\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Loaded dataframe\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(filepath)\n",
    "        print(f\"Data loaded successfully: {self.data.shape[0]} rows, {self.data.shape[1]} columns\")\n",
    "        return self.data\n",
    "    \n",
    "    def preprocess_data(self, target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Preprocess the data for training.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        target_col : str, default='cardio'\n",
    "            Name of the target variable column\n",
    "        drop_cols : list, default=None\n",
    "            List of columns to drop (e.g., ID columns)\n",
    "        \"\"\"\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"No data loaded. Call load_data() first.\")\n",
    "        \n",
    "        self.drop_cols = drop_cols if drop_cols else []\n",
    "        \n",
    "        df = self.data.copy()\n",
    "        if drop_cols:\n",
    "            df = df.drop(columns=drop_cols, errors='ignore')\n",
    "        \n",
    "        X = df.drop(columns=[target_col])\n",
    "        y = df[target_col]\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=self.test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        if self.scale_features:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X_train = pd.DataFrame(\n",
    "                self.scaler.fit_transform(self.X_train),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_train.index\n",
    "            )\n",
    "            self.X_test = pd.DataFrame(\n",
    "                self.scaler.transform(self.X_test),\n",
    "                columns=self.feature_names,\n",
    "                index=self.X_test.index\n",
    "            )\n",
    "        \n",
    "        print(f\"Data preprocessed: Train size = {len(self.X_train)}, Test size = {len(self.X_test)}\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the Tabular CNN classifier.\"\"\"\n",
    "        if self.X_train is None:\n",
    "            raise ValueError(\"Data not preprocessed. Call preprocess_data() first.\")\n",
    "        \n",
    "        # Convert to tensors with channel dimension for Conv1D\n",
    "        X_train_tensor = torch.FloatTensor(self.X_train.values).unsqueeze(1).to(self.device)  # (N, 1, features)\n",
    "        y_train_tensor = torch.LongTensor(self.y_train.values).to(self.device)\n",
    "        \n",
    "        self.X_train_tensor = X_train_tensor\n",
    "        self.y_train_tensor = y_train_tensor\n",
    "        \n",
    "        # Define model\n",
    "        input_size = len(self.feature_names)\n",
    "        conv_layers = []\n",
    "        prev_channels = 1\n",
    "        prev_size = input_size\n",
    "        for f, k in zip(self.filters, self.kernel_sizes):\n",
    "            conv_layers.append(nn.Conv1d(prev_channels, f, kernel_size=k, padding=(k-1)//2))\n",
    "            conv_layers.append(nn.ReLU())\n",
    "            conv_layers.append(nn.MaxPool1d(2))\n",
    "            prev_channels = f\n",
    "            prev_size = prev_size // 2  # Approximate after pooling\n",
    "        \n",
    "        # Flatten size calculation (approximate)\n",
    "        flatten_size = prev_channels * (input_size // 4)  # Assuming two poolings halve twice\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *conv_layers,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flatten_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        # DataLoader\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            running_loss = 0.0\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                avg_loss = running_loss / len(train_loader)\n",
    "                print(f'Epoch [{epoch+1}/{self.epochs}], Loss: {avg_loss:.4f}')\n",
    "        \n",
    "        print(\"Model trained successfully!\")\n",
    "    \n",
    "    def predict(self, X=None):\n",
    "        \"\"\"\n",
    "        Make predictions on data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pd.DataFrame, default=None\n",
    "            Data to predict. If None, uses test set.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray : Predictions\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            if X is None:\n",
    "                input_tensor = torch.FloatTensor(self.X_test.values).unsqueeze(1).to(self.device)\n",
    "            else:\n",
    "                if self.scale_features and self.scaler is not None:\n",
    "                    X_scaled = pd.DataFrame(\n",
    "                        self.scaler.transform(X),\n",
    "                        columns=X.columns,\n",
    "                        index=X.index\n",
    "                    )\n",
    "                    input_tensor = torch.FloatTensor(X_scaled.values).unsqueeze(1).to(self.device)\n",
    "                else:\n",
    "                    input_tensor = torch.FloatTensor(X.values).unsqueeze(1).to(self.device)\n",
    "            \n",
    "            outputs = self.model(input_tensor)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            return predicted.cpu().numpy()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"\n",
    "        Evaluate model performance on test set.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing all performance metrics\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        y_pred = self.predict()\n",
    "        \n",
    "        self.metrics = {\n",
    "            'accuracy': accuracy_score(self.y_test, y_pred),\n",
    "            'precision': precision_score(self.y_test, y_pred, average='binary'),\n",
    "            'recall': recall_score(self.y_test, y_pred, average='binary'),\n",
    "            'f1_score': f1_score(self.y_test, y_pred, average='binary'),\n",
    "            'confusion_matrix': confusion_matrix(self.y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"MODEL PERFORMANCE METRICS\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Accuracy:  {self.metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {self.metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {self.metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {self.metrics['f1_score']:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(self.metrics['confusion_matrix'])\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(self.y_test, y_pred, \n",
    "                                   target_names=['No Disease', 'Disease']))\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return self.metrics\n",
    "    \n",
    "    def export_predictions(self, output_filepath, include_all_data=True):\n",
    "        \"\"\"\n",
    "        Export predictions to a new CSV file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        output_filepath : str\n",
    "            Path for the output CSV file\n",
    "        include_all_data : bool, default=True\n",
    "            If True, includes all original columns. If False, only includes\n",
    "            the target and predicted columns.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        X_full = self.data.copy()\n",
    "        \n",
    "        if self.drop_cols:\n",
    "            X_full = X_full.drop(columns=self.drop_cols, errors='ignore')\n",
    "        \n",
    "        X_full = X_full.drop(columns=['cardio'], errors='ignore')\n",
    "        \n",
    "        if self.scale_features and self.scaler is not None:\n",
    "            X_full_scaled = pd.DataFrame(\n",
    "                self.scaler.transform(X_full),\n",
    "                columns=self.feature_names,\n",
    "                index=X_full.index\n",
    "            )\n",
    "            predictions = self.predict(X_full_scaled)\n",
    "        else:\n",
    "            predictions = self.predict(X_full)\n",
    "        \n",
    "        if include_all_data:\n",
    "            output_df = self.data.copy()\n",
    "        else:\n",
    "            output_df = pd.DataFrame()\n",
    "            output_df['cardio'] = self.data['cardio']\n",
    "        \n",
    "        output_df['predicted'] = predictions\n",
    "        \n",
    "        output_df.to_csv(output_filepath, index=False)\n",
    "        print(f\"Predictions exported to: {output_filepath}\")\n",
    "        \n",
    "        return output_df\n",
    "    \n",
    "    def run_full_pipeline(self, input_filepath, output_filepath, \n",
    "                         target_col='cardio', drop_cols=None):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline: load, preprocess, train, evaluate, and export.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_filepath : str\n",
    "            Path to input CSV file\n",
    "        output_filepath : str\n",
    "            Path for output CSV file with predictions\n",
    "        target_col : str, default='cardio'\n",
    "            Name of target variable\n",
    "        drop_cols : list, default=None\n",
    "            Columns to drop before training\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Performance metrics\n",
    "        \"\"\"\n",
    "        print(\"Starting full pipeline...\\n\")\n",
    "        \n",
    "        self.load_data(input_filepath)\n",
    "        self.preprocess_data(target_col=target_col, drop_cols=drop_cols)\n",
    "        self.train()\n",
    "        metrics = self.evaluate()\n",
    "        self.export_predictions(output_filepath)\n",
    "        \n",
    "        print(\"Pipeline completed successfully!\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # MLP Example\n",
    "    print(\"Running MLP Pipeline...\")\n",
    "    mlp_clf = CardioMLPClassifier(\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        hidden_sizes=[128, 64, 32],\n",
    "        lr=0.0005,\n",
    "        epochs=150,\n",
    "        batch_size=64,\n",
    "        scale_features=True\n",
    "    )\n",
    "    \n",
    "    matrix_mlp = mlp_clf.run_full_pipeline(\n",
    "        input_filepath='train.csv',\n",
    "        output_filepath='Prediction_MLP.csv',\n",
    "        drop_cols=['id'] \n",
    "    )\n",
    "    \n",
    "    # CNN Example\n",
    "    # print(\"\\nRunning Tabular CNN Pipeline...\")\n",
    "    # cnn_clf = CardioTabularCNNClassifier(\n",
    "    #     test_size=0.2,\n",
    "    #     random_state=42,\n",
    "    #     filters=[32, 64],\n",
    "    #     kernel_sizes=[3, 3],\n",
    "    #     lr=0.001,\n",
    "    #     epochs=100,\n",
    "    #     batch_size=32,\n",
    "    #     scale_features=True\n",
    "    # )\n",
    "    \n",
    "    # cnn_metrics = cnn_clf.run_full_pipeline(\n",
    "    #     input_filepath='cardio_train.csv',\n",
    "    #     output_filepath='cardio_train_cnn_pred.csv',\n",
    "    #     drop_cols=['id'] \n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff940438",
   "metadata": {},
   "source": [
    "# Final Voted Ensamble algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbddb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LogisticRegression': {'file': 'Prediction_LogisticRegression.csv', 'metrics': {'accuracy': 0.7249951352403191, 'precision': 0.7508342602892102, 'recall': 0.6641739643805963, 'f1_score': 0.7048504150785778, 'confusion_matrix': array([[8153, 2240],\n",
      "       [3413, 6750]])}}, 'RandomForest': {'file': 'Prediction_RandomForest.csv', 'metrics': {'accuracy': 0.7298598949211909, 'precision': 0.7464314354450682, 'recall': 0.6869372693726937, 'f1_score': 0.7154496541122214, 'confusion_matrix': array([[5348, 1581],\n",
      "       [2121, 4654]])}}, 'NaiveBayes': {'file': 'Prediction_NaiveBayes.csv', 'metrics': {'accuracy': 0.7071657910099242, 'precision': 0.7500905469032959, 'recall': 0.6113653136531365, 'f1_score': 0.673660242335529, 'confusion_matrix': array([[5549, 1380],\n",
      "       [2633, 4142]])}}, 'GradientBoosting': {'file': 'Prediction_GradientBoosting.csv', 'metrics': {'accuracy': 0.7247518972562755, 'precision': 0.7400479616306954, 'recall': 0.6832472324723248, 'f1_score': 0.7105141980046048, 'confusion_matrix': array([[5303, 1626],\n",
      "       [2146, 4629]])}}, 'SVM': {'file': 'Prediction_SVM.csv', 'metrics': {'accuracy': 0.732778750729714, 'precision': 0.7605020920502092, 'recall': 0.6707011070110701, 'f1_score': 0.7127843137254902, 'confusion_matrix': array([[5498, 1431],\n",
      "       [2231, 4544]])}}, 'KNN': {'file': 'Prediction_KNN.csv', 'metrics': {'accuracy': 0.6697314652656159, 'precision': 0.6698897114367729, 'recall': 0.6544649446494465, 'f1_score': 0.6620875018665074, 'confusion_matrix': array([[4744, 2185],\n",
      "       [2341, 4434]])}}, 'MLP': {'file': 'Prediction_MLP.csv', 'metrics': {'accuracy': 0.7221249270286048, 'precision': 0.7468796804792811, 'recall': 0.6624354243542435, 'f1_score': 0.7021276595744681, 'confusion_matrix': array([[5408, 1521],\n",
      "       [2287, 4488]])}}}\n",
      "============================================================\n",
      "WEIGHTED ENSEMBLE VOTING FOR CARDIO PREDICTION\n",
      "============================================================\n",
      "\n",
      "1. Loading prediction files...\n",
      "Successfully loaded 7 model predictions\n",
      "Number of samples: 68520\n",
      "\n",
      "2. Performing weighted ensemble voting...\n",
      "\n",
      "=== Model Weights (Based on Accuracy) ===\n",
      "SVM: 0.1462 (Accuracy: 0.7328)\n",
      "RandomForest: 0.1456 (Accuracy: 0.7299)\n",
      "LogisticRegression: 0.1447 (Accuracy: 0.7250)\n",
      "GradientBoosting: 0.1446 (Accuracy: 0.7248)\n",
      "MLP: 0.1441 (Accuracy: 0.7221)\n",
      "NaiveBayes: 0.1411 (Accuracy: 0.7072)\n",
      "KNN: 0.1336 (Accuracy: 0.6697)\n",
      "\n",
      "3. Calculating ensemble metrics...\n",
      "\n",
      "=== Ensemble Model Performance ===\n",
      "Accuracy:  0.7572\n",
      "Precision: 0.7967\n",
      "Recall:    0.6833\n",
      "F1-Score:  0.7357\n",
      "\n",
      "Confusion Matrix:\n",
      "[[28736  5907]\n",
      " [10729 23148]]\n",
      "  TN: 28736, FP: 5907\n",
      "  FN: 10729, TP: 23148\n",
      "\n",
      "=== Comparison with Individual Models ===\n",
      "             Model  Accuracy  Precision   Recall  F1-Score\n",
      "LogisticRegression  0.724995   0.750834 0.664174  0.704850\n",
      "      RandomForest  0.729860   0.746431 0.686937  0.715450\n",
      "        NaiveBayes  0.707166   0.750091 0.611365  0.673660\n",
      "  GradientBoosting  0.724752   0.740048 0.683247  0.710514\n",
      "               SVM  0.732779   0.760502 0.670701  0.712784\n",
      "               KNN  0.669731   0.669890 0.654465  0.662088\n",
      "               MLP  0.722125   0.746880 0.662435  0.702128\n",
      "  *** ENSEMBLE ***  0.757210   0.796696 0.683295  0.735651\n",
      "\n",
      "Improvement over best individual model: +0.0244\n",
      "Successfully loaded 7 model predictions\n",
      "Number of samples: 68520\n",
      "\n",
      "2. Performing weighted ensemble voting...\n",
      "\n",
      "=== Model Weights (Based on Accuracy) ===\n",
      "SVM: 0.1462 (Accuracy: 0.7328)\n",
      "RandomForest: 0.1456 (Accuracy: 0.7299)\n",
      "LogisticRegression: 0.1447 (Accuracy: 0.7250)\n",
      "GradientBoosting: 0.1446 (Accuracy: 0.7248)\n",
      "MLP: 0.1441 (Accuracy: 0.7221)\n",
      "NaiveBayes: 0.1411 (Accuracy: 0.7072)\n",
      "KNN: 0.1336 (Accuracy: 0.6697)\n",
      "\n",
      "3. Calculating ensemble metrics...\n",
      "\n",
      "=== Ensemble Model Performance ===\n",
      "Accuracy:  0.7572\n",
      "Precision: 0.7967\n",
      "Recall:    0.6833\n",
      "F1-Score:  0.7357\n",
      "\n",
      "Confusion Matrix:\n",
      "[[28736  5907]\n",
      " [10729 23148]]\n",
      "  TN: 28736, FP: 5907\n",
      "  FN: 10729, TP: 23148\n",
      "\n",
      "=== Comparison with Individual Models ===\n",
      "             Model  Accuracy  Precision   Recall  F1-Score\n",
      "LogisticRegression  0.724995   0.750834 0.664174  0.704850\n",
      "      RandomForest  0.729860   0.746431 0.686937  0.715450\n",
      "        NaiveBayes  0.707166   0.750091 0.611365  0.673660\n",
      "  GradientBoosting  0.724752   0.740048 0.683247  0.710514\n",
      "               SVM  0.732779   0.760502 0.670701  0.712784\n",
      "               KNN  0.669731   0.669890 0.654465  0.662088\n",
      "               MLP  0.722125   0.746880 0.662435  0.702128\n",
      "  *** ENSEMBLE ***  0.757210   0.796696 0.683295  0.735651\n",
      "\n",
      "Improvement over best individual model: +0.0244\n",
      "\n",
      "4. Final predictions saved to: Prediction_Ensemble_Weighted.csv\n",
      "\n",
      "=== Prediction Summary ===\n",
      "Total predictions: 68520\n",
      "predicted cardio=0: 39465 (57.60%)\n",
      "predicted cardio=1: 29055 (42.40%)\n",
      "\n",
      "============================================================\n",
      "ENSEMBLE VOTING COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "\n",
      "4. Final predictions saved to: Prediction_Ensemble_Weighted.csv\n",
      "\n",
      "=== Prediction Summary ===\n",
      "Total predictions: 68520\n",
      "predicted cardio=0: 39465 (57.60%)\n",
      "predicted cardio=1: 29055 (42.40%)\n",
      "\n",
      "============================================================\n",
      "ENSEMBLE VOTING COMPLETED SUCCESSFULLY\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "# Model configuration\n",
    "models_config = {\n",
    "    'LogisticRegression': {\n",
    "        'file': 'Prediction_LogisticRegression.csv',\n",
    "        'metrics': matrix_logisticregression\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'file': 'Prediction_RandomForest.csv',\n",
    "        'metrics': matrix_randomforest\n",
    "    },\n",
    "    'NaiveBayes': {\n",
    "        'file': 'Prediction_NaiveBayes.csv',\n",
    "        'metrics': matrix_naivebayes\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'file': 'Prediction_GradientBoosting.csv',\n",
    "        'metrics': matrix_gradientboosting\n",
    "    },\n",
    "    'SVM': {\n",
    "        'file': 'Prediction_SVM.csv',\n",
    "        'metrics': matrix_svm\n",
    "    },\n",
    "    'KNN': {\n",
    "        'file': 'Prediction_KNN.csv',\n",
    "        'metrics': matrix_knn\n",
    "    },\n",
    "    'MLP': {\n",
    "        'file': 'Prediction_MLP.csv',\n",
    "        'metrics': matrix_mlp\n",
    "    }\n",
    "}\n",
    "print(models_config)\n",
    "def load_predictions():\n",
    "    \"\"\"Load all prediction files and extract predicted column\"\"\"\n",
    "    predictions_dict = {}\n",
    "    base_df = None\n",
    "    \n",
    "    for model_name, config in models_config.items():\n",
    "        try:\n",
    "            df = pd.read_csv(config['file'])\n",
    "            \n",
    "            # Store base dataframe (all columns except predicted)\n",
    "            if base_df is None:\n",
    "                base_df = df.drop(columns=['predicted']) if 'predicted' in df.columns else df.copy()\n",
    "            \n",
    "            # Extract predictions\n",
    "            if 'predicted' in df.columns:\n",
    "                predictions_dict[model_name] = df['predicted'].values\n",
    "            else:\n",
    "                print(f\"Warning: 'predicted' column not found in {config['file']}\")\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: File {config['file']} not found. Skipping {model_name}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {config['file']}: {str(e)}\")\n",
    "    \n",
    "    return base_df, predictions_dict\n",
    "\n",
    "def weighted_voting(predictions_dict, models_config):\n",
    "    \"\"\"\n",
    "    Perform weighted voting based on model accuracies\n",
    "    \n",
    "    Args:\n",
    "        predictions_dict: Dictionary of model predictions\n",
    "        models_config: Configuration with accuracy metrics\n",
    "    \n",
    "    Returns:\n",
    "        final_predictions: Array of final predictions\n",
    "        weights: Dictionary of normalized weights\n",
    "    \"\"\"\n",
    "    # Extract accuracies and create weights\n",
    "    weights = {}\n",
    "    for model_name in predictions_dict.keys():\n",
    "        accuracy = models_config[model_name]['metrics']['accuracy']\n",
    "        weights[model_name] = accuracy\n",
    "    \n",
    "    # Normalize weights to sum to 1\n",
    "    total_weight = sum(weights.values())\n",
    "    normalized_weights = {k: v/total_weight for k, v in weights.items()}\n",
    "    \n",
    "    print(\"\\n=== Model Weights (Based on Accuracy) ===\")\n",
    "    for model, weight in sorted(normalized_weights.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{model}: {weight:.4f} (Accuracy: {models_config[model]['metrics']['accuracy']:.4f})\")\n",
    "    \n",
    "    # Perform weighted voting\n",
    "    n_samples = len(next(iter(predictions_dict.values())))\n",
    "    weighted_votes = np.zeros(n_samples)\n",
    "    \n",
    "    for model_name, predictions in predictions_dict.items():\n",
    "        weight = normalized_weights[model_name]\n",
    "        weighted_votes += predictions * weight\n",
    "    \n",
    "    # Final prediction: if weighted vote >= 0.5, predict 1, else 0\n",
    "    final_predictions = (weighted_votes >= 0.5).astype(int)\n",
    "    \n",
    "    return final_predictions, normalized_weights\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate comprehensive metrics\"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"WEIGHTED ENSEMBLE VOTING FOR CARDIO PREDICTION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load all predictions\n",
    "    print(\"\\n1. Loading prediction files...\")\n",
    "    base_df, predictions_dict = load_predictions()\n",
    "    \n",
    "    if not predictions_dict:\n",
    "        print(\"Error: No prediction files loaded successfully.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Successfully loaded {len(predictions_dict)} model predictions\")\n",
    "    print(f\"Number of samples: {len(next(iter(predictions_dict.values())))}\")\n",
    "    \n",
    "    # Perform weighted voting\n",
    "    print(\"\\n2. Performing weighted ensemble voting...\")\n",
    "    final_predictions, weights = weighted_voting(predictions_dict, models_config)\n",
    "    \n",
    "    # Add final predictions to dataframe\n",
    "    final_df = base_df.copy()\n",
    "    final_df['predicted'] = final_predictions\n",
    "    \n",
    "    # Calculate ensemble metrics if ground truth is available\n",
    "    if 'cardio' in final_df.columns:\n",
    "        print(\"\\n3. Calculating ensemble metrics...\")\n",
    "        ensemble_metrics = calculate_metrics(final_df['cardio'], final_predictions)\n",
    "        \n",
    "        print(\"\\n=== Ensemble Model Performance ===\")\n",
    "        print(f\"Accuracy:  {ensemble_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {ensemble_metrics['precision']:.4f}\")\n",
    "        print(f\"Recall:    {ensemble_metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score:  {ensemble_metrics['f1_score']:.4f}\")\n",
    "        print(f\"\\nConfusion Matrix:\")\n",
    "        print(ensemble_metrics['confusion_matrix'])\n",
    "        print(f\"  TN: {ensemble_metrics['confusion_matrix'][0,0]}, FP: {ensemble_metrics['confusion_matrix'][0,1]}\")\n",
    "        print(f\"  FN: {ensemble_metrics['confusion_matrix'][1,0]}, TP: {ensemble_metrics['confusion_matrix'][1,1]}\")\n",
    "        \n",
    "        # Compare with individual models\n",
    "        print(\"\\n=== Comparison with Individual Models ===\")\n",
    "        comparison_data = []\n",
    "        for model_name, config in models_config.items():\n",
    "            if model_name in predictions_dict:\n",
    "                comparison_data.append({\n",
    "                    'Model': model_name,\n",
    "                    'Accuracy': config['metrics']['accuracy'],\n",
    "                    'Precision': config['metrics']['precision'],\n",
    "                    'Recall': config['metrics']['recall'],\n",
    "                    'F1-Score': config['metrics']['f1_score']\n",
    "                })\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Model': '*** ENSEMBLE ***',\n",
    "            'Accuracy': ensemble_metrics['accuracy'],\n",
    "            'Precision': ensemble_metrics['precision'],\n",
    "            'Recall': ensemble_metrics['recall'],\n",
    "            'F1-Score': ensemble_metrics['f1_score']\n",
    "        })\n",
    "        \n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        print(comparison_df.to_string(index=False))\n",
    "        \n",
    "        # Check if ensemble improved\n",
    "        max_individual_accuracy = max([config['metrics']['accuracy'] for config in models_config.values()])\n",
    "        improvement = ensemble_metrics['accuracy'] - max_individual_accuracy\n",
    "        print(f\"\\nImprovement over best individual model: {improvement:+.4f}\")\n",
    "    \n",
    "    # Save final predictions\n",
    "    output_file = 'Prediction_Ensemble_Weighted.csv'\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\n4. Final predictions saved to: {output_file}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n=== Prediction Summary ===\")\n",
    "    print(f\"Total predictions: {len(final_predictions)}\")\n",
    "    print(f\"predicted cardio=0: {np.sum(final_predictions == 0)} ({np.sum(final_predictions == 0)/len(final_predictions)*100:.2f}%)\")\n",
    "    print(f\"predicted cardio=1: {np.sum(final_predictions == 1)} ({np.sum(final_predictions == 1)/len(final_predictions)*100:.2f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ENSEMBLE VOTING COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
